#Base Imports
import os
import math
import itertools
import pandas as pd
import numpy as np
from tpot import TPOTRegressor

#Plotting Utilities
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.colors as colors
from mpl_toolkits.axes_grid1 import AxesGrid
from mpl_toolkits.axes_grid1 import make_axes_locatable
plt.rcParams['font.size']=42

from scipy.stats import norm,chi2,t
from scipy.optimize import differential_evolution

#Import Scikit learn functions
from sklearn.model_selection import cross_val_score,cross_val_predict
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFECV,RFE
from sklearn.preprocessing import FunctionTransformer
from sklearn.decomposition import PCA
from sklearn.exceptions import ConvergenceWarning

#Import Models
from sklearn.svm import SVR
from sklearn.ensemble import BaggingRegressor, RandomForestRegressor
from sklearn.linear_model import BayesianRidge
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.linear_model import LassoLarsIC
from sklearn.gaussian_process import GaussianProcessRegressor

#For Rendering
from IPython.display import display,HTML
import pdfkit

#Handle Convergence Warnings
import warnings
warnings.filterwarnings("ignore", category=ConvergenceWarning)

#=========================================================#
# Helper Functions                                        #
#---------------------------------------------------------#

#Helper Functions
def is_number(s):
    '''Return true if the input is a number.'''
    try:
        float(s)
        return True
    except ValueError:
        return False


def dist(X,Y):
    '''Calculate Euclidean Distance between two points in R**n'''
    return math.sqrt(sum([(x-y)**2 for x,y in zip(X,Y)]))


def plot_model_predictions(name,predicted,actual,log=False,ax=None):
    if log:
        predicted = [math.log(x) for x in predicted]
        actual = [math.log(y) for y in actual]

    #mu,sigma = calculate_moments(actual,predicted)
    #r2,pval = pearsonr(predicted,actual)
    #mse = mean_squared_error(predicted,actual)
    #print(name,'R^2: ', r2,'p-val: ',pval,'MSE: ',mse)
    if ax is None:
        fig = plt.figure()
        ax = plt.gca()
        
    ax.scatter(predicted,actual)
    ax.set_title(name + ' Predicted vs. Actual')

    #Plot Correct Ranges
    padding_y = (max(actual) - min(actual))*0.1
    ax.set_ylim(min(actual)-padding_y,max(actual)+padding_y)

    padding_x = (max(predicted) - min(predicted))*0.1
    ax.set_xlim(min(predicted)-padding_x,max(predicted)+padding_x)

    ax = plt.gca()
    ax.plot([-150,150], [-150,150], ls="--", c=".3")

    ax.set_xlabel('Predicted ' + name)
    ax.set_ylabel('Actual ' + name)

    

def create_bounds(features,padding_factor=1.2,negativeAllowed=False):
    features_transpose = list(map(list, zip(*features)))
    bounds = []
    for dim in range(len(features[0])):
        maxVal = max(features_transpose[dim])
        minVal = min(features_transpose[dim])
        mu = (maxVal + minVal)/2
        shift = (maxVal - minVal) * padding_factor / 2
        
        if negativeAllowed or mu - shift > 0:
            bounds.append((mu-shift,mu+shift))
        else:
            bounds.append((0.1*(mu+shift),mu+shift))
    return bounds


def plot_model(model,data,targets,midpoint=0.1,title=None,zlabel=None,ax=None,pcs=None,plot_points=True):
    '''Plots a 2d projection of the model onto the principal components.
       The data is overlayed onto the model for visualization.
    
       model is a sklearn model
    '''
    
    #Visualize Model
    #Create Principal Compoenents for Visualiztion of High Dimentional Space
    pca = PCA(n_components=2)
    if pcs is not None:
        pca.components_ = pcs
    
    data_transformed = pca.fit_transform(data)
    
    
    #Get Data Range
    xmin = np.amin(data_transformed[:,0])
    xmax = np.amax(data_transformed[:,0])
    ymin = np.amin(data_transformed[:,1])
    ymax = np.amax(data_transformed[:,1])

    #Scale Plot Range
    scaling_factor = 0.5
    xmin = xmin - (xmax - xmin)*scaling_factor/2
    xmax = xmax + (xmax - xmin)*scaling_factor/2
    ymin = ymin - (ymax - ymin)*scaling_factor/2
    ymax = ymax + (ymax - ymin)*scaling_factor/2

    #Generate Points in transformed Space
    points = 1000
    x = np.linspace(xmin,xmax,num=points)
    y = np.linspace(ymin,ymax,num=points)
    xv, yv = np.meshgrid(x,y)

    #reshape data for inverse transform
    xyt = np.concatenate((xv.reshape([xv.size,1]),yv.reshape([yv.size,1])),axis=1)
    xy = pca.inverse_transform(xyt)
    
    #Make Sure No Values are below 0
    zero_truncate = np.vectorize(lambda x: max(0.01,x))
    xy = zero_truncate(xy)
    
    #predict z values for plot
    z = model.predict(xy).reshape([points,points])
    minpoint = min([min(p) for p in z])
    maxpoint = max([max(p) for p in z])
    
    #Plot Contour from Model
    if ax is None:
        fig = plt.figure()
        ax = plt.gca()
    
    scaled_targets = [target/max(targets)*200 for target in targets]
    
    #Overlay Scatter Plot With Training Data
    if plot_points:
        ax.scatter(data_transformed[:,0],
                    [1*value for value in data_transformed[:,1]],
                    c='k',
                    cmap=plt.cm.bwr,
                    marker='+',
                    s=scaled_targets,
                    linewidths=1.5
                    )
    
    ax.grid(b=False)

    midpercent = (midpoint-minpoint)/(maxpoint-minpoint)
    centered_cmap = shiftedColorMap(plt.cm.bwr, midpoint=midpercent)
    cmap = centered_cmap
    
    if midpercent > 1:
        midpercent = 1
        cmap = plt.cm.Blues_r
    elif midpercent < 0:
        midpercent = 0
        cmap = plt.cm.Reds
    
    z = [row for row in reversed(z)]
    im = ax.imshow(z,extent=[xmin,xmax,ymin,ymax],cmap=cmap)
    ax.set_aspect('auto')
    
    if title is not None:
        ax.set_title(title)
    
    ax.set_xlabel('Principal Component 1')
    ax.set_ylabel('Principal Component 2')

    # create an axes on the right side of ax. The width of cax will be 5%
    # of ax and the padding between cax and ax will be fixed at 0.05 inch.
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="5%", pad=0.05)
    if zlabel is not None:
        plt.colorbar(im, cax=cax,label=zlabel)
    else:
        plt.colorbar(im, cax=cax)
   
        
def shiftedColorMap(cmap, start=0, midpoint=0.5, stop=1.0, name='shiftedcmap'):
    '''
    Function to offset the "center" of a colormap. Useful for
    data with a negative min and positive max and you want the
    middle of the colormap's dynamic range to be at zero

    Input
    -----
      cmap : The matplotlib colormap to be altered
      start : Offset from lowest point in the colormap's range.
          Defaults to 0.0 (no lower ofset). Should be between
          0.0 and `midpoint`.
      midpoint : The new center of the colormap. Defaults to 
          0.5 (no shift). Should be between 0.0 and 1.0. In
          general, this should be  1 - vmax/(vmax + abs(vmin))
          For example if your data range from -15.0 to +5.0 and
          you want the center of the colormap at 0.0, `midpoint`
          should be set to  1 - 5/(5 + 15)) or 0.75
      stop : Offset from highets point in the colormap's range.
          Defaults to 1.0 (no upper ofset). Should be between
          `midpoint` and 1.0.
    '''
    cdict = {
        'red': [],
        'green': [],
        'blue': [],
        'alpha': []
    }

    # regular index to compute the colors
    reg_index = np.linspace(start, stop, 257)

    # shifted index to match the data
    shift_index = np.hstack([
        np.linspace(0.0, midpoint, 128, endpoint=False), 
        np.linspace(midpoint, 1.0, 129, endpoint=True)
    ])

    for ri, si in zip(reg_index, shift_index):
        r, g, b, a = cmap(ri)

        cdict['red'].append((si, r, r))
        cdict['green'].append((si, g, g))
        cdict['blue'].append((si, b, b))
        cdict['alpha'].append((si, a, a))

    newcmap = colors.LinearSegmentedColormap(name, cdict)
    plt.register_cmap(cmap=newcmap)

    return newcmap


def ball_target_moments(point,engineering_accuracy,model,sample_points=10,confidence_level=0.95):
    #Generate Interval for Random Samples
    low_bound = [value - value*engineering_accuracy for value in point]
    high_bound = [value + value*engineering_accuracy for value in point]
    #print(low_bound,high_bound)
    #Generate uniform random sample points inside of those boundaries 
    samples = [[np.random.uniform(low=low_bound[i],high=high_bound[i]) for i in range(len(low_bound))] for j in range(sample_points)]
    
    #Predict sample points
    predictions = []
    predictions.append(model.predict(samples).tolist())
    
    #Calculate sample mean & confidence intervals
    k = sample_points - 1 #Degrees of freedom
    s = np.std(predictions)
    m = np.mean(predictions)
    s_m = s / math.sqrt(k + 1)
    #print(s,m)
    mean_interval = [m - t.ppf(0.975,k)*s_m,m + t.ppf(0.975,k)*s_m]

    #print(k,t.ppf(0.025,k))

    #Calculate sample standard deviation & confidence intervals
    interval = [math.sqrt(k*s**2/chi2.ppf(1-confidence_level/2,k)),math.sqrt(k*s**2/chi2.ppf(confidence_level/2,k))]

    #Print Relevant Stats
    percent_error = lambda x: (np.mean(x) - x[0])/np.mean(x)
    #print('Mean:',m,mean_interval,percent_error(mean_interval))
    #print('Stdev:',s,interval,percent_error(interval))

    mu = m
    sigma = max(interval)
    
    return mu,sigma

#Functions which generate markdown
def create_figure_markdown(fig,caption,fname):
    '''Create Markdown and the correct files for a Figure'''
    base_url = os.path.dirname(os.path.realpath('../tmp'))
    
    #Save Figure to File in Temp Folder (Should be Unique Name)
    fig.savefig('../tmp/' + fname)
    plt.close(fig)
    url = base_url + '/tmp/' + fname
    
    #Create HTML for Displaying that Figure
    html = '<div class=\'figure\'> <img src=\'' + url + '\' >\n' 
    html += '<p>' + caption + '</p></div>\n'
    return html

def summarize_success_probability(self):
    success_prob = 0
    threshold = 0.9
    for i,probability in enumerate(self.probabilities):
        success_prob += (1-success_prob)*(probability/100)
        if success_prob > threshold:
            break
    
    html = '<p><b>(Left)</b> Success Probability Analysis: In order to have a ' + str(round(threshold*100)) + '% chance of successfully meeting the stated objective above ' + str(i+1) + ' strains must be constructed.'
    html += ' <b>(Right)</b> Strains Selected: Here the black pluses show the points analyzed in the data set.  Larger Plus means more production.  Blue plusses show predicted strains.</p>'
    return html

def summarize_model_statistics(self):
    html  = '<p> The following models have been evaluated:'
    for model in self.models:
        html += str(model) + ',' 
    html += '.'
    
    for model in self.models:
        html += 'The ' + str(model) + ' residuals have a standard deviation in log space of '
        html += str(self.residual_stdev[model])
        html += ' and the mean is shifted by ' + str(self.residual_mean[model])
    html += '</p>'
    return html
    

#==========================================================#
# Define Default ML Models                                 #
#----------------------------------------------------------#

polynomialRegressor = Pipeline([('Scaler',StandardScaler()),
                                ('Polynomial Features',PolynomialFeatures(degree=2, include_bias=True, interaction_only=True)),
                                ('Feature Reduction',RFECV(Ridge(),cv=10, scoring='r2')),
                                ('Linear Regressor',BaggingRegressor(base_estimator=Ridge(),
                                                                     n_estimators=100, max_samples=.8,
                                                                     bootstrap=False,
                                                                     bootstrap_features=False,
                                                                     random_state=None))])

supportVectorRegressor = Pipeline([('Scaler',StandardScaler()),
                                   ('SVR',SVR())])

LogScale = FunctionTransformer(np.log1p)
LogRegressor = Pipeline([('Log Transform',LogScale),('Scaler',StandardScaler()),
                                   ('Linear',LassoLarsIC())])


#=========================================================#
# Reccomendation Class                                    #
#---------------------------------------------------------#

class recommendation_engine():
    models = {
              'Random Forest Regressor':RandomForestRegressor(),
              'Log Regressor':LogRegressor,
              'Polynomial Regressor':polynomialRegressor,
             }
    
    prediction_metrics = {}
    residual_stdev = {}
    residual_mean = {}
    
    def __init__(self,
                 data_file = None,               # The EDD Data File Path 
                 phase_space_file = None,        # The Phase Space Data File Path
                 strain_file = None,             # The Stain Input file Relating Strains to phase space definitions
                 features=None,                  # List of Strings of the Data Features
                 target=None,                    # Target Feature String
                 objective_dict=None,            # Dictonary of the Objective
                 num_strains=3,                  # Number of Strain Predictions to Return
                 engineering_accuracy = None,    # Accuarcy of being able to hit Proteomic Parameters
                 verbose=False,                  # Display Information while Running?
                 engineering_base_strains=None   # Base Strains If Engineering to target Proteomics
                ):
        
        
        #Load Objective Dict
        #TODO: Check that the right fields are in objective_dict
        if objective_dict is not None:
            self.objective = objective_dict['objective']
            self.threshold = objective_dict['threshold']
        
        self.engineering_base_strains = engineering_base_strains
        
        #Based on Inputs and input contents, Set Tasks to Complete
        if target is None:
            if phase_space_file is not None:
                #=======================================================================#
                # Phase Space File With No Target: Build a Strain File For Construction #
                #-----------------------------------------------------------------------#
                print('No Target Supplied. Building a Set of Initial Strains to Construct from Phase Space File.')
                self.generate_initial_strains()
                
            else:
                print('No Target Supplied. Please Supply a Phase Space File if you wish to construct a set of initial strains')
                raise ValueError()
                
        else:
            self.target = target
            if phase_space_file is None and data_file is not None:
                #===============================================================#
                # Target with Only Data File Specified:                         #
                # Make Predictions Based On Protoemics And Metabolomics Targets #
                #---------------------------------------------------------------#
            
                '''Open and Process EDD data file'''
                self.edd_df = pd.read_csv(data_file)

                #Extract Line Names, Measurement Types, and Time Points
                self.line_names = self.edd_df['Line Name'].unique() 
                self.measurement_types = self.edd_df['Measurement Type'].unique()
                self.time_points = [ candidate_point for candidate_point in self.edd_df.columns.values if is_number(candidate_point)]

                #Reshape EDD dataframe to contain only data measurment types and line names for each time point
                self.edd_df = self.edd_df[['Line Name','Measurement Type'] + self.time_points]
                time_df = pd.DataFrame(data=self.time_points,columns=['Time Points',])
                self.edd_df = pd.pivot_table(self.edd_df,values=time_df['Time Points'],index=['Line Name'],columns='Measurement Type',aggfunc=np.sum)

                #self.set_parameters(features,target,objective_dict)
                

                
                #Todo: (Figure out What to do with multiple time points!)
                if len(self.time_points) == 1:    
                    self.features = [(self.time_points[0],feature) for feature in features]
                    self.target = (self.time_points[0],target[0])
                    #(self.features,self.target)
                    #display(self.edd_df)
                    self.X = self.edd_df.as_matrix(columns=[(self.features)])
                    self.y = self.edd_df[self.target].values
                    #display(self.edd_df)
                else:
                    raise ValueError('Multiple Time Point Data Not Currently Supported!')
                    
                #Predict Target Numbers to Hit... (Create a different function for hitting strains from definition file)
                self.predict_features(max_results=num_strains,disp=verbose,engineering_accuracy=engineering_accuracy)
            
            elif phase_space_file is not None and data_file is not None:
                #========================================#
                # Phase Space File & Data File Specified #
                # Predict Strains from Data              # 
                #----------------------------------------#
                
                #Load Files
                self.phase_space_df = pd.read_csv(phase_space_file)
                self.data_df = pd.read_csv(data_file)
                if strain_file is not None:
                    self.strain_df = pd.read_csv(strain_file)
                else:
                    raise ValueError('No Strain File to Make Predictions. Please include Strain File')
                    
                #Create Strain Part Dict
                strain_parts = set(map(tuple, self.strain_df[['Order','Part']].values))
                self.strain_part_dict = dict()

                for order,part in strain_parts:
                    ids = self.phase_space_df['ID'].loc[self.phase_space_df['Type'] == part].unique().tolist()
                    self.strain_part_dict[order] = ids
                            
                print(self.strain_part_dict)
                    
                #Check to see if there is feature data for parts
                if self.part_features_exist() and self.is_proteomics():
                    #TODO: Validate Features (Not yet Implemented)
                    pass

                elif self.part_features_exist():
                    #TODO: Add Copy Number Support (Add Copy Numbers to feature vector)
                    print('Create Maren Features!')
                    #Get List of Strains
                    temp_df = self.strain_df.set_index(['Order','Line Name']).unstack('Order')
                    strain_parts = temp_df['ID'].values
                    strain_id = temp_df.index.values
                    print(strain_id)
                    self.X = self.strains_to_features(strain_parts)
                    self.y = self.strains_to_target(strain_id)
                    
                elif self.is_proteomics():
                    #TODO: If feature data is incomplete see if there is proteomics data to predict feature_strength 
                    pass
                    
                else:
                    #TODO: If there is not enough data do Catagorical features and alert the user
                    print('Fitting Using Catagorical Features')
                    pass
                
                #Create Feature Target Pairs in a dataframe to train models
                self.predict_strains(max_results=num_strains,disp=verbose)
                    
                

    def part_features_exist(self):
        features = ['Promoter','Terminator','Coding Sequence','RBS']
        part_features = self.phase_space_df['Feature 1'].loc[self.phase_space_df['Type'].isin(features)].values
        return not any(np.isnan(part_features))
    
    
    def is_proteomics(self):
        if 'Targeted Proteomics' in self.data_df['Protocol Name'].unique():
            return True
        else:
            return False
        
    
    def generate_initial_strains(self,n):
        '''Create a list of n initial designs based on the provided phase space, return the list, and part usage statistics'''
        #Open Parts CSV from J5
        
        #Get List of Parts and enumerate statistics
        pass
        
    
    def __cross_validate(self,model):
        '''Calculate Error Residuals for Model For Reporting using 10-fold Cross Validation'''
        y_predict = cross_val_predict(self.models[model],self.X,y=self.y,cv=10)
        score = cross_val_score(self.models[model],self.X,y=self.y,cv=10)
        y_error = [y_a - y_p for y_a,y_p in zip(self.y,y_predict)]
        #y_error = [math.log(max(y_a,0.01)) - math.log(max(y_p,0.01)) for y_a,y_p in zip(self.y,y_predict)]
        #y_error = [[(y_p - y_a)/y_a for y_a,y_p in zip(self.y,y_predict)]]
        self.prediction_metrics[model] = {'predictions':y_predict,
                                          'residuals':y_error}
        self.residual_stdev[model] = np.std(y_error)
        self.residual_mean[model] = np.mean(y_error)
        
        
    def visualize_model_fits(self,models):
        '''Create Plots for a Set of Models'''
        n = len(models)
        scale = 1.6
        fig = plt.figure(figsize=(15*scale,4*n*scale),dpi=300)
        for i,model in enumerate(models):
            
            ax = fig.add_subplot(n,3,3 + i*3)
            sns.distplot(self.prediction_metrics[model]['residuals'],ax=ax)
            ax.set_title(model + ' Error Residuals')
            
            ax = fig.add_subplot(n,3,2 + i*3)
            plot_model_predictions('Limonene',self.prediction_metrics[model]['predictions'],self.y,ax=ax)

            ax = fig.add_subplot(n,3,1 + i*3)
            plot_model(self.models[model],self.X,self.y,zlabel=self.target[0]+ ' (mg/L)',title=model + ' fit',ax=ax)
        
        plt.tight_layout()
        return fig
        
        
    def __visualize_predictions(self,predictions,model,ax=None):
        '''Visualize Predictions In Comparison to The Data Set'''
        
        # Calculate Prediction Results
        #print(predictions)
        predicted_outputs = self.models[model].predict(predictions).tolist()
        all_states  =  predictions + self.X.tolist()
        all_outputs =  predicted_outputs + self.y.tolist()
        
        # Create Composite PCA with both Training Set & Predictions
        pca = PCA(n_components=2)
        data_transformed = pca.fit_transform(all_states)
        
        # Plot the figure
        n = len(predictions)
        if ax is None:
            fig = plt.figure()
            ax = plt.gca()
        
        plot_model(self.models[model],all_states,all_outputs,zlabel=self.target[0]+ ' (mg/L)',title=model + ' fit',plot_points=False,ax=ax)
        scaled_targets = [target/max(all_outputs)*200 for target in all_outputs]

        
        ax.scatter(data_transformed[0:n,0],
                    data_transformed[0:n,1],
                    c='b',
                    marker='+',
                    s=scaled_targets[0:n],
                    linewidths=1.5)
        
        ax.scatter(data_transformed[n:,0],
                    data_transformed[n:,1],
                    c='k',
                    marker='+',
                    s=scaled_targets[n:],
                    linewidths=1.5)
        
        #Add PCA Components explained variation to axis labels
        explained_variation = [str(round(var*100)) for var in pca.explained_variance_ratio_]
        ax.set_xlabel('Principal Component 1 (' + explained_variation[0] + '% of Variance Explained)')
        ax.set_ylabel('Principal Component 2 (' + explained_variation[1] + '% of Variance Explained)')
        
        #return fig
    
    
    def __visualize_data(self):
        #Look for High Correlations with good R values
        
        #Plot Correlations Automatically on a scatter plot
        pass
    
    
    def __visualize_success_probability(self,probabilities,ax=None):
        cprob = [0,]
        failure_prob = 100
        if ax is None:
            fig = plt.figure()
            ax = plt.gca()
        
        for prob in probabilities:
            failure_prob *= (100 - prob)/100
            cprob.append(100-failure_prob)
        ax.plot(cprob)    
    
    
    def visualize_predicted_strains(self):
        scale = 1.6
        fig = plt.figure(figsize=(7.5*3*scale,5*scale),dpi=300)
        
        #Plot Success Probability
        ax = fig.add_subplot(1,2,1)
        self.__visualize_success_probability(self.probabilities,ax=ax)
        ax.set_title('Success Probability vs. Number of Strains Constructed')
        ax.set_xlabel('Number of Predicted Strains Constructed')
        ax.set_ylabel('Probability of at least one Strain Meeting Specification')
        
        
        #Plot Placement of new Predictions
        ax = fig.add_subplot(1,2,2)
        self.__visualize_predictions(self.predictions,self.best_model,ax=ax)
        ax.set_title('New Strain Predictions Overlayed on Best Model')
        
        return fig
    
    
    def set_parameters(self,features,target,objective_dict):
        #ToDo: Error Checking on Inputs
        
        #Set Variables
        self.parameters_set = True
        self.features = features
        self.target = target
        self.objective = objective_dict['objective']
        self.threshold = objective_dict['threshold']
        #self.production_target = production_target    
    
    
    def fit(self,log_features=False):
        '''Fit Data to Specified Models using provided feature & target labels'''
        
        #feature_indecies = [(self.time_points[0],feature_name) for feature_name in self.features]
        #target_indecies = [(self.time_points[0],target_name) for target_name in self.target]

        #self.X = self.edd_df.as_matrix(columns=feature_indecies).tolist()
        if log_features:
            log_mat = np.vectorize(math.log)
            self.X = log_mat(self.X)
 
        #self.y = self.edd_df.as_matrix(columns=target_indecies).transpose().tolist()[0]
        #Use Tpot to find models
        self.X = np.array(self.X)
        self.y = np.array(self.y)
        
        #Fit TPOT Model
        self.models['TPOT'] = TPOTRegressor(generations=20, population_size=100, cv=5, verbosity=2)
        self.models['TPOT'] = self.models['TPOT'].fit(self.X, self.y).fitted_pipeline_
        print(self.models['TPOT'].score(self.X, self.y))
        
        #Cross Validate & Fit Models
        for model in self.models:
            self.__cross_validate(model)
            self.models[model].fit(self.X,self.y)
        
        self.visualize_model_fits(self.models)
        plt.show()
    
    
    def success_prob(self,model,point,target_interval,engineering_accuracy=0.10):
        '''Calculate the probability of hitting a target interval given a point in the feature space.
           Engineering error is incorporated into the calculation so that nearby points are taken into account.
        '''
        
        #Gather Predictions in an evenly spaced ball about the target point based on engineering accuracy
        #Sample until estimates of mean and standard deviation are known to 1% error in 95% confidence intervals
        #Report mean & worst case standard deviation at 95% convidence intervals        
            
        #Calculate the Moments of the Distribution for Engineering
        if engineering_accuracy == 0:
            mu = point
            sigma = 0
        else:
            mu,sigma = ball_target_moments(point,engineering_accuracy,self.models[model],sample_points=100)
                            
        sig = math.sqrt(sigma**2 + self.residual_stdev[model]**2)
        mu = mu + self.residual_mean[model]
            
        #Calculate The Probability of hitting the target interval (Add the influence of the point P)
        prob = (norm.sf(target_interval[0], mu, sig) - norm.sf(target_interval[1], mu, sig))*100
    
        return prob
    
    
    def strains_to_features(self,strain_parts):
        '''Take a list of parts and return the feature vector associated with that strain'''
        #Calculate Features from Strain Lists
        X = [ ] 
        for strain in strain_parts:
            features = []
            for part_id in strain:
                #Get Non NaN features
                feature = self.phase_space_df[['Feature 1','Feature 2','Feature 3','Feature 4']].loc[self.phase_space_df['ID']==part_id].values[0]
                features += feature[~np.isnan(feature)].tolist()
            X.append(features)
        
        return X
    
    def strains_to_target(self,strain_ids):
        #Get Time Points
        time_points = [col for col in self.data_df.columns if is_number(col)]
        y = self.data_df[time_points[0]].values#.loc[self.data_df['Measurement Type']==self.target]
        return y
    
    def predict_strains(self,max_results=10,disp=False):
        
        #Fit Models
        self.fit()

        #Find Best Model
        self.best_model = min(self.residual_stdev, key=self.residual_stdev.get)
        
        #Find the Success Probability Function given the problem objective
        target_interval = {}
        success_prob = {}
        for model in self.models:
            target_interval[model] = self.calculate_target_interval(model)
            success_prob[model] = lambda p: self.success_prob(model,p,target_interval[model],0)
        
        self.strains = []
        self.probabilities = []
        self.production = []
        self.predictions = []
        for i in range(max_results):
            
            #Find bounds for Strains for differential evolution
            #Define Cost Given Currently Selected Points
            def cost(x):
                #Round off Strain Input
                strain = tuple([self.strain_part_dict[i+1][int(part_num)] for i,part_num in enumerate(x)])
                #print(strain)
                
                #Check To make sure the strain is not the same as others in the set
                if strain in self.strains:
                    return 0
            
                #Convert Strain ID into Features
                features = self.strains_to_features([strain,])
                
                #Check to see success prob if not within dist threshold
                prob = 0
                n = len(self.models)
                for model in self.models:
                    p = self.models[model].predict(np.array(features).reshape(1, -1))[0]
                    prob += success_prob[model](p) / n
                return -1*prob
        
            #Calculate Bounds for Strain Parts
            bounds = [(0,len(self.strain_part_dict[key])) for key in self.strain_part_dict]
            sol = differential_evolution(cost,bounds,disp=disp)
            
            strain = tuple([self.strain_part_dict[i+1][int(part_num)] for i,part_num in enumerate(sol.x)])
            features = self.strains_to_features([strain,])
            
            self.strains.append(list(strain))
            self.probabilities.append(-1*sol.fun)
            self.predictions.append(features[0])
            self.production.append(self.models[self.best_model].predict(np.array(features).reshape(1, -1))[0])
        
        # From Set of Strains Create Dataframe with probability of Success
        columns = ['Part ' + str(key) for key in self.strain_part_dict] + ['Success Probability'] + [self.target,]
        data = [feature + [prob,] + [target,] for feature,prob,target in zip(self.strains,self.probabilities,self.production)]
        self.prediction_df = pd.DataFrame(data=data,columns=columns)
        #display(self.prediction_df)
        
        
    def predict_features(self,max_results=10,disp=False,engineering_accuracy=0.1):
        #Make Sure Parameters are Set
        if True:
            
            #Fit Models
            self.fit()
            
            #Find Best Model 
            self.best_model = min(self.residual_stdev, key=self.residual_stdev.get)
            
            
            
            # Generate a set of predictions to meet objective
            bounds = create_bounds(self.X)
            points = []
            probs = []
            production = []
            min_distance_between_points = self.distance_distribution(self.X)
            
            #Calculate Success Probability Functions for each model
            target_interval = {}
            success_prob = {}
            for model in self.models:
                target_interval[model] = self.calculate_target_interval(model)
                success_prob[model] = lambda p: self.success_prob(model,p,target_interval[model],engineering_accuracy)
                
            #print(target_interval)
            for i in range(max_results):
                
                #Define Cost Given Currently Selected Points
                def cost(x):
                    
                    #Check to see if it is within distance threshold
                    if len(points) > 0: 
                        if min(self.point_distance(x,points)) <= min_distance_between_points:
                            return 0
            
                    #Check to see success prob if not within dist threshold
                    prob = 0
                    n = len(self.models)
                    for model in self.models:
                        p = self.models[model].predict(np.array(x).reshape(1, -1))[0]
                        #print(x,p,target_interval[model],success_prob[model](p))
                        prob += success_prob[model](p) / n
                    
                    return -1*prob
                
                sol = differential_evolution(cost,bounds,disp=disp)
                points.append(sol.x.tolist())
                probs.append(-1*sol.fun)
                production.append([self.models[model].predict(np.array(sol.x).reshape(1, -1))[0] for model in self.models])
            
            # Put Results into Dataframe for Export calculate error and spread for each one... 
            columns = self.features + ['Success Probability'] + [model for model in self.models]
            data = [feature + [prob,] + target for feature,prob,target in zip(points,probs,production)]
            self.probabilities = probs
            self.predictions = points
            self.prediction_df = pd.DataFrame(data=data,columns=columns)
            
            if disp:
                display(self.prediction_df)
    
    def calculate_target_interval(self,model):
        '''Figure out what the goal interval is for declaring success given a model'''
        
        
        if self.objective == 'maximize':
            #Set Success Condition and Cost for Interval Search
            cost = lambda x: -1*self.models[model].predict(np.array(x).reshape(1, -1))[0]
            success_val = max(self.y)*(1+self.threshold)
            
            #Find Value Which Minimizes Cost
            sol = differential_evolution(cost,create_bounds(self.X),disp=False)
            target_interval = [min(success_val,-1*sol.fun),math.inf]
        else:
            raise ValueError('Only Maximize Objective is Currently Implemented')     
        
        return target_interval    
        
    
    def generate_report(self,output_file=None):
        '''Generate a PDF report of the ART Output'''
        
        #First Generate HTML
        markdown = ''
                
        # Generate Header & Intro Paragraph Based on Objective
        if self.objective == 'minimize':
            markdown += '<h1>Minimize ' + self.target[-1] + ' Production</h1>\n'
                
        elif self.objective == 'maximize':
            markdown += '<h1>Predictions to Maximize ' + self.target[-1] + ' Production</h1>\n'
            markdown += '<p>In this data set the maximum value is ' + str(round(max(self.y))) + ' (mg/L) of ' + self.target[-1] + '.  '
            markdown += 'The objective is to predict strain designs which will maximize production.  '
            markdown += 'Successful design is to exceed the maximum observed production by ' + str(round(self.threshold*100)) + '% which is ' + str(round((1 + self.threshold)*max(self.y))) + ' (mg/L).</p>\n'
        
        elif self.objective == 'target':
            markdown += '<h1>Target ' + str(self.production) + '(mg/L) ' + self.target[-1] + ' Production</h1>\n'
        
        #Figure 1. Strain Probabilities Visualization
        fig1 = self.visualize_predicted_strains()
        caption1 = summarize_success_probability(self)
        
        if output_file is None:
            display(HTML(markdown))
            plt.show(fig1)
            markdown = ''
            markdown += caption1
        else:
            markdown += create_figure_markdown(fig1,caption1,'figure1.png')
                
        #Table 1. A List of all strains
        markdown += '<h2>Predictions</h2>'
        #markdown += '<p>First as a point of comparision, the best performing strain in the dataset is presented.</p>'
        #markdown += '<div class=\'table\'>' + self.edd_df.loc[self.edd_df[self.time_points[0],self.target[0]] == max(self.y)].to_html() + '</div>'
        markdown += '<p>The Table Below provides a set of predictions which if followed should maximize the chance of successful strain engineering.</p>'
        markdown += '<div class=\'table\'>' + self.prediction_df.to_html() + '</div>'
        
        #Table 2. (If there are engineering Base Strains Specified) Calculate Changes to base closest base strain needed...
        #TODO: If needed Remove Brute Force Approach to finding closest strain (Once strain numbers get large)
        if self.engineering_base_strains is not None:
            
            columns = ['Base Strain'] + self.features
            data = []
            engineering_df = self.edd_df[self.features].loc[self.edd_df.index.isin(self.engineering_base_strains)]
            engineering_strains = engineering_df.values
            engineering_strain_names = engineering_df.index.values
            predicted_strains = self.prediction_df[self.features].values
            for target_strain in predicted_strains:
                
                #find closest engineering strain
                min_cost = math.inf
                min_strain = None
                min_strain_name = None
                for strain_name,base_strain in zip(engineering_strain_names,engineering_strains):
                    cost = lambda X,Y: sum([(x-y)**2 for x,y in zip(X,Y)])
                    if cost(target_strain,base_strain) < min_cost:
                        min_strain = base_strain
                        min_strain_name = strain_name
                    
                #Calculate Fold Changes Needed To Reach Target Strain
                def fold_change(base_strain, target_strain):
                    fold_changes = []
                    for base_val,target_val in zip(base_strain,target_strain):
                        pos_neg = 1
                        if base_val > target_val:
                            pos_neg = -1
                        
                        fold_changes.append(pos_neg*max(base_val,target_val)/min(base_val,target_val))
                    return fold_changes
                
                
                line = [min_strain_name] + fold_change(min_strain,target_strain)
                data.append(line)
            
            engineering_df = pd.DataFrame(data,columns=columns)
            
            markdown += '<h2>Engineering Guidelines</h2>'
            markdown += '<p>The Table Below provides instructions on how to use the base strains to most easily reach the new predictions. Fold changes are expressed for each protein with respect to the base strain listed.</p>'
            markdown += '<div class=\'table\'>' + engineering_df.to_html() + '</div>'
            
        
            #Add Data Frame to HTML for Printing
            
            
            
            
        
        #Figure 2. Model Fit Plots with Caption Statistitics 
        fig2 = self.visualize_model_fits(self.models)
        caption2 = summarize_model_statistics(self)
        markdown += '<h2>Model Evaluation</h2>'
        if output_file is None:
            display(HTML(markdown))
            plt.show(fig2)
            markdown = ''
            markdown += caption2
        else:
            markdown += create_figure_markdown(fig2,caption2,'figure2.png')
        
        #Figure 3. Data Relationships (Future)        
        
        #Print to Jupyter Notebook if output file is none
        if output_file is None:
            display(HTML(markdown))
        else:
            #Figure out extension
            _ , extension = os.path.splitext(output_file)
            extension = extension[1:].lower()
            if extension == 'pdf':
                #Render PDF from HTML with CSS Report Stylesheet
                options = {
                        'dpi':96,
                        'page-size': 'Letter',
                        'margin-top': '0.75in',
                        'margin-right': '0.75in',
                        'margin-bottom': '0.75in',
                        'margin-left': '0.75in',
                        'encoding': "UTF-8",
                        'no-outline': None,
                        }

                pdfkit.from_string(markdown, output_file, options=options, css='../AutomaticRecommendationTool/report.css') #, css=stylesheet

            elif extension == 'csv':
                self.prediction_df.to_csv(output_file)
                
            else:
                raise ValueError('Invalid File Extension. Cannot Save out Report in Format:' + str(extension))
    
    

    def point_distance(self,X,points):
        '''Calculate Distance between one point and a set of points'''
        return [dist(X,Y) for Y in points]
        
    
    def distance_distribution(self,points,display=False,threshold=0.6):
        '''Visualize the distances between all of the proteomic points to find a good threshold for guesses'''
        distances = sorted([dist(x,y) for x, y in itertools.combinations(points, 2)])
        if display:
            sns.distplot(distances)
            plt.show()
        min_dist_index = int(len(distances)*(1-threshold))
        return distances[min_dist_index]